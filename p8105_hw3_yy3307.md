p8105_hw3_yy3307
================
Yang Yi
2022-10-07

``` r
library(tidyverse)
library(ggridges)
library(patchwork)

library(p8105.datasets)

knitr::opts_chunk$set(
    echo = TRUE,
    warning = FALSE,
    fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Problem 1

The first step we need to do is importing the `instacart` data from
`p8105.datasets`.

``` r
data("instacart")

instacart = 
  instacart %>% 
  as_tibble(instacart)
```

*The `Instacart` dataset contains data of online grovery services which
allow peopleto shop online from local stores. The dataset has 1384617
rows and 15 columns, which includes variables of
`order_id, product_id, add_to_cart_order, reordered, user_id, eval_set, order_number, order_dow, order_hour_of_day, days_since_prior_order, product_name, aisle_id, department_id, aisle, department`.
We can make some data analysis with this dataset, for instance, we can
indicate which type of grocery is most popular by counting frequencies
of `aisle` variable. Also, we can observe something like the
`reorder rate`, the `reorder rate` of this dataset is 0.5985944, which
means that about 59.86% of all customers would like to reorder.*

We can use `count` and `distinct` functions to find the total number of
aisles. And we also need to return the `name` of aisle which most
repeated being ordered, `which.max` can help us find that aisle. The
most occurrence aisle is easy to find, but we need to `filter` the aisle
column to eliminate the most occurrence aisle first and again use
`which.max` to find the second most occurrence aisle from the new list
of aisles.

``` r
num_aisle = count(distinct(instacart, aisle))

aisle_most = names(which.max(table(instacart$aisle)))

except_fresh =
  instacart %>%
  filter(!grepl('fresh vegetables', aisle))
aisle_2most = names(which.max(table(except_fresh$aisle)))
```

*There are **134** distinct aisles, **fresh vegetables** and **fresh
fruits** are the most items ordered from.*

To make a plot that we want, the first step is to manipulate our dataset
with conditions. In this condition, we need to `count` how many items
are ordered in each aisle and `filter` out how many of them has more
than 10000 items ordered. We can call `fct_reorder` from `forcats`
package to change the order level according to aisle name and number of
orders. Then, we can plot out the number of items ordered in each aisle
using `geom_point` with labels and themes being added. Lastly, we just
need to save the output plot into a `results` directory using `ggsave`.

``` r
reorder_aisle =
  instacart %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n))

numorder_aisle =
  ggplot(data = reorder_aisle, aes(x = aisle, y = n)) + 
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1)) +
  theme(plot.title = element_text(hjust = 0.5))

ggsave("./results/problem_1/aisle_orders.pdf", numorder_aisle, width = 8, height = 5)
numorder_aisle
```

<img src="p8105_hw3_yy3307_files/figure-gfm/plot aisle-1.png" width="90%" />

Then we are going to make a table showing the top three popular items in
each of “baking ingredients”, “dog food care”, and “packaged vegetables
fruits” aisles. The table will include corresponding aisle, product
name, number of orders, and rank.

``` r
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```

| aisle                      | product_name                                  |    n | rank |
|:---------------------------|:----------------------------------------------|-----:|-----:|
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |

*From the table we can indicate that the top three most popular items
for “packaged vegetables fruits” are `Organic Baby Spinach`,
`Organic Raspberries`, and `Organic Blueberries`.The top three most
popular items for “baking ingredients” are `Light Brown Sugar`,
`Pure Baking Soda`, and `Cane Sugar`. The top three most popular items
for “dog food care” are `Snack Sticks Chicken & Rice Recipe Dog Treats`,
`Organix Chicken & Brown Rice Recipe`, and `Small Dog Biscuits`.*

Next we’ll create a table showing the mean hour of orders in each
weekday for “Pink Lady Apples” and “Coffee Ice Cream”. We can first
`filter` and `group by` product name and order dow and then `summarize`
the mean order hour for each weekday. Since we want to create a 2\*7
table, we can use `pivot_wider` to change the spread of data.

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_order_hour = mean(order_hour_of_day)) %>%
  pivot_wider(names_from = "order_dow", values_from = "mean_order_hour") %>%
  knitr::kable(digits = 2)
```

| product_name     |     0 |     1 |     2 |     3 |     4 |     5 |     6 |
|:-----------------|------:|------:|------:|------:|------:|------:|------:|
| Coffee Ice Cream | 13.77 | 14.32 | 15.38 | 15.32 | 15.22 | 12.26 | 13.83 |
| Pink Lady Apples | 13.44 | 11.36 | 11.70 | 14.25 | 11.55 | 12.78 | 11.94 |

*This result table infers that Pink Lady Apples are generally purchased
slightly earlier in the day than Coffee Ice Cream, with the exception of
day 5.*

# Problem 2

First we need to import, clean and wrangle the `accel_data` dataset.
Since the original dataset has 1440 activities listed in wide forma, I
would like to use `pivot_longer` to transform these activities with
their values into a column format with name equals `activity` and value
equals to `activity_count`. Then, we’ll need a “weekday vs. weekend”
column. We can `mutate` this column using the `case_when` function to
determine which days should be considered as “weekday” and which days
are “weekend”. Since one-minute intervals are common for
`activity_minutes`, I’ll delete the “activity” prefix and change the
class of this variable to `integers`.

``` r
accel_df =
  read.csv(file = "./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  pivot_longer(
    activity_1:activity_1440,
    names_to = "activity_minutes",
    values_to = "activity_counts",
    names_prefix = "activity_"
  ) %>%
  mutate(
    weekday_vs_weekend = case_when(
      day == "Monday"    ~ "weekday",
      day == "Tuesday"   ~ "weekday",
      day == "Wednesday" ~ "weekday",
      day == "Thursday"  ~ "weekday",
      day == "Friday"    ~ "weekday",
      day == "Saturday"  ~ "weekend",
      day == "Sunday"    ~ "weekend",
      TRUE               ~ ""
    )
  ) %>%
  mutate(activity_minutes = as.integer(activity_minutes))
```

*After manipulating the “accel” dataset, it contains 50400 rows and 6
columns, which means that there are 50400 observations. We have
`week, day_id, day, activity_minutes, activity_counts, weekday_vs_weekend`
as variables, where `week, day_id, activity_minutes, activity_counts`
are numerical variables and `day, activity, weekday_vs_weekend` in
character variables.*

Then we want to focus on analyzing total activity over each day. First
we need to group our data by variables according to each day, then we
need to create a `total_activity` variable to sum up minutes of all
activities. With these information, we can generate a table showing
totals for each day.s

``` r
accel_df %>%
  group_by(week, day_id, day) %>%
  summarize(total_activity = sum(activity_counts)) %>%
  knitr::kable()
```

| week | day_id | day       | total_activity |
|-----:|-------:|:----------|---------------:|
|    1 |      1 | Friday    |      480542.62 |
|    1 |      2 | Monday    |       78828.07 |
|    1 |      3 | Saturday  |      376254.00 |
|    1 |      4 | Sunday    |      631105.00 |
|    1 |      5 | Thursday  |      355923.64 |
|    1 |      6 | Tuesday   |      307094.24 |
|    1 |      7 | Wednesday |      340115.01 |
|    2 |      8 | Friday    |      568839.00 |
|    2 |      9 | Monday    |      295431.00 |
|    2 |     10 | Saturday  |      607175.00 |
|    2 |     11 | Sunday    |      422018.00 |
|    2 |     12 | Thursday  |      474048.00 |
|    2 |     13 | Tuesday   |      423245.00 |
|    2 |     14 | Wednesday |      440962.00 |
|    3 |     15 | Friday    |      467420.00 |
|    3 |     16 | Monday    |      685910.00 |
|    3 |     17 | Saturday  |      382928.00 |
|    3 |     18 | Sunday    |      467052.00 |
|    3 |     19 | Thursday  |      371230.00 |
|    3 |     20 | Tuesday   |      381507.00 |
|    3 |     21 | Wednesday |      468869.00 |
|    4 |     22 | Friday    |      154049.00 |
|    4 |     23 | Monday    |      409450.00 |
|    4 |     24 | Saturday  |        1440.00 |
|    4 |     25 | Sunday    |      260617.00 |
|    4 |     26 | Thursday  |      340291.00 |
|    4 |     27 | Tuesday   |      319568.00 |
|    4 |     28 | Wednesday |      434460.00 |
|    5 |     29 | Friday    |      620860.00 |
|    5 |     30 | Monday    |      389080.00 |
|    5 |     31 | Saturday  |        1440.00 |
|    5 |     32 | Sunday    |      138421.00 |
|    5 |     33 | Thursday  |      549658.00 |
|    5 |     34 | Tuesday   |      367824.00 |
|    5 |     35 | Wednesday |      445366.00 |

*In general, the trends of total activity over the day are not apparent,
because total activity fluctuates irregularly between days. But we can
find some extreme small values on 24th and 31st day, both of them are
Saturday.*

Lastly we are going to create a single-panel plot showing the
correlation between `activity minutes` after midnight and
`activity counts` and use color to indicate day of the week.

``` r
activity_hour_plot =
  accel_df %>%
    ggplot(aes(x = activity_minutes, y = activity_counts, color = day)) +
    geom_point() +
    labs(
      title = "24-Hour Activity Time Courses For Each Day",
      x = "Time After Midnight (Min)",
      y = "Activity Counts",
      caption = "Data from accel_data"
    ) +
    theme(plot.title = element_text(hjust = 0.5))

ggsave("./results/problem_2/activity_hour.pdf", activity_hour_plot, width = 8, height = 5)
activity_hour_plot
```

<img src="p8105_hw3_yy3307_files/figure-gfm/accel plot-1.png" width="90%" />

*We find that for most days in a week, most of the activity counts show
a clear upward trend from 6:00 a.m. to 12:00 p.m., among them, activity
counts on Thursday and Sunday peaked during this time period. From 12:30
pm to 18:45 pm, activity counts fluctuate in a moderate range for most
days, and activity counts on Saturdays and Sundays reach higher
increases around 16:40 pm. For Wednesday, Friday, and Saturday,
20:50p.m. is when activity counts fluctuate the most. We can conclude
that for most days in a week, activity counts It is basically at a
stable minimum value in the early morning, which may be caused by deep
sleep. And it will reach its peak twice at noon and at night.*

# Problem 3

Import ny_noaa dataset:

``` r
data("ny_noaa")

ny_noaa =
  ny_noaa %>%
  as_tibble(ny_noaa)
```

*Before manipulating the `ny_noaa` dataset, we have `2595176` rows and
`7` columns of data, including column variables of
`id, date, prcp, snow, snwd, tmax, tmin`. `prcp, snow, snwd` represent
precipitation, snowfall, and snow depth for the following day.
`tmax and tmin` indicates maximum and minimum temperature. There is a
large amount of missing data, which are indicated by “na”. This will
affect the summarization of the data, because if we exclude all missing
data when calculating important parameters such as mean, sd, median,
etc., then our data will lose validity and power.*

We can do some data cleaning to the `ny_noaa` dataset and `separate` the
`date` variable into `year, month, day` format. `tmax and tmin`
variables are in character class so we need to convert them into integer
format, then we need to `mutate` `prcp, tmax, tmin` variables in order
to keep the units as tenths of mm/degrees C. To find the most commonly
observed value for snowfall, again we can use `which.max` function to
find the value of most occurrence.

``` r
noaa_df =
  ny_noaa %>%
    janitor::clean_names() %>%
    separate(date, into = c("year", "month", "day"), sep = "-") %>%
    mutate(
      prcp = prcp/10,
      tmax = as.integer(tmax) / 10,
      tmin = as.integer(tmin) / 10
    )

snowfall_obs = names(which.max(table(noaa_df$snow)))
snow_most_count = as.integer(length(which(noaa_df$snow == 0)))
```

*We can tell that the most commonly observed value for snowfall is “r
snowfall_obs”, which occurs 2008508 times. This is because `ny_noaa`
records the weather conditions for days in almost every month, snow
usually occurs in winter and most of the time there is no snow, so the
snowfall for these times will be 0 mm.*

I’m going to make a two-panel plot showing the average max temperature
in January and in July in each station across years.

``` r
noaa_df %>%
  group_by(id, year,month) %>% 
  filter(month == "01" | month == "07") %>% 
  summarize(
    mean_tmax = mean(tmax,na.rm = TRUE)
  ) %>% 
  mutate(
    month = recode(month, "01" = "January", "07" = "July"),
    year = as.numeric(year)
  ) %>%
  ggplot(aes(x = year,y = mean_tmax,color = month)) +
  geom_point() +
  geom_smooth() +
  labs(
    title = "Average Max Temperature in January vs July from 1981 to 2010",
    x = "Year",
    y = "Average Max Temperature (tenths of degrees C)",
    caption = "Data from NY NOAA"
  ) +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_grid(~month) 
```

<img src="p8105_hw3_yy3307_files/figure-gfm/plot max temp-1.png" width="90%" />

*Average maximum temperature in January from 1981 to 2010 fluctuates up
and down roughly around 0 degree C on a five-year cycle, while average
maximum temperature in July from 1981 to 2010 overall stable at 27
degrees C. There exist some outliers average maximum temperature for
both January and July, for instance, the lowest temperature of January
2005 is about -12 degree C which is far away from rests data in the same
month.*
